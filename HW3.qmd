---
title: "Homework 3: Namit Shrivastava"
format: pdf
editor: visual
---

## 1) The following are cluster totals $$y_\alpha$$ from a = 20 clusters of exactly b = 10 women (between the ages of 15 and 24) each. These clusters and the young women were sampled from the population frame used for Homework 1. The cluster totals $$y_\alpha$$ are the number of women who have ever been pregnant. Assume that the clusters were selected at random and with replacement, and the students were selected with epsem and without replacement. The sampling fraction is f = ab/AB = n/N = 200/2,920 = 1 / 14.6, meaning that the finite population correction (fpc) should not be ignored in this case. 

| Cluster | $y_\alpha$ |
|:--------|:----------:|
| 1       | 4          |
| 2       | 4          |
| 3       | 3          |
| 4       | 6          |
| 5       | 4          |
| 6       | 6          |
| 7       | 3          |
| 8       | 4          |
| 9       | 4          |
| 10      | 1          |
| 11      | 1          |
| 12      | 8          |
| 13      | 3          |
| 14      | 3          |
| 15      | 5          |
| 16      | 6          |
| 17      | 4          |
| 18      | 5          |
| 19      | 8          |
| 20      | 5          |

## a) Compute an estimate of the mean , y bar its standard error, and a 95% confidence interval for the population mean. (Hint: the degrees of freedom used in computing this confidence interval should not be 199.)

Now for cluster sampling, the estimate of the population mean is:

$$\bar{y} = \frac{1}{ab}\sum_{\alpha=1}^a y_\alpha$$

where:
- $a$ is the number of clusters (20)
- $b$ is the cluster size (10)
- $y_\alpha$ is the cluster total

The variance of the mean is:

$$V(\bar{y}) = \frac{1}{a^2b^2}\sum_{\alpha=1}^a(y_\alpha - \bar{y}b)^2/(a-1)$$

And the standard error is:

$$SE(\bar{y}) = \sqrt{V(\bar{y})}$$

So step by step calculation will be:

Mean: $$\bar{y} = \frac{87}{(20 \times 10)} = 0.435$$

Variance: $$V(\bar{y}) = \frac{1}{20^2}\sum_{\alpha=1}^{20}(y_\alpha/10 - 0.435)^2/19$$

Standard Error: $$SE(\bar{y}) = \sqrt{V(\bar{y})}$$

95% Confidence Interval: $$\bar{y} \pm t_{19,0.975} \times SE(\bar{y})$$

```{r}
# Data
y_alpha <- c(4, 4, 3, 6, 4, 6, 3, 4, 4, 1, 1, 8, 3, 3, 5, 6, 4, 5, 8, 5)
a <- 20  # number of clusters
b <- 10  # cluster size
N <- 2920  # total population size
n <- a * b  # sample size

# Calculating mean
y_bar <- sum(y_alpha)/(a*b)

# Calculating variance and SE with fpc
cluster_means <- y_alpha/b
var_y <- var(cluster_means)  # using a-1 degrees of freedom
fpc <- sqrt((N - n) / N)
se_y <- sqrt(var_y/a) * fpc

# Calculating 95% CI using t-distribution with a-1 df
t_value <- qt(0.975, df = a-1)
ci_lower <- y_bar - t_value * se_y
ci_upper <- y_bar + t_value * se_y

cat("Mean:", y_bar, "\n")
cat("Standard Error:", se_y, "\n")
cat("95% CI:", c(ci_lower, ci_upper), "\n")
```
Based on the analysis of the cluster sampling data, I found that the estimated mean number of women who have ever been pregnant in each cluster of 10 women is approximately 4.35. 

The standard error of this estimate is about 0.040, which gives an idea of the variability of the sample mean. 

Using this standard error, I calculated a 95% confidence interval for the population mean, which ranges from approximately 3.50 to 5.20 women per cluster. This means one can be 95% confident that the true mean number of women who have ever been pregnant in each cluster of 10 women falls within this interval.

Also the relatively narrow confidence interval suggests that the estimate is fairly precise, even when accounting for the finite population correction. This precision indicates that the sampling method was effective in capturing the variability within the population.


## b) Estimate the standard error of the mean that you would expect if the sample consisted of a = 40 clusters of b = 10 each. (Hint: What about this design has not changed, and what quantity needed to answer this question could therefore be considered portable?)

For cluster sampling with a different number of clusters, I can use the fact that the cluster means variance remains constant. The new standard error would be:

$$SE_{new}(\bar{y}) = \sqrt{\frac{V(y_\alpha/b)}{a_{new}}} \times \sqrt{\frac{N-n_{new}}{N}}$$

where:
- $V(y_\alpha/b)$ is the variance of cluster means (portable from previous calculation)
- $a_{new} = 40$ is the new number of clusters
- $n_{new} = a_{new} \times b$ is the new total sample size
- $N = 2920$ is the total population size

So a step by step calculation will be:

Using portable variance of cluster means: $$V(y_\alpha/b) = 0.0326$$

Calculating new standard error: $$SE_{new} = \sqrt{\frac{0.0326}{40}} \times \sqrt{\frac{2920-400}{2920}} = 0.0267$$

```{r}
# Taking Data from previous calculation
var_cluster_means <- var(y_alpha/b)  # portable variance of cluster means
a_new <- 40  # new number of clusters
b <- 10      # cluster size (unchanged)
N <- 2920    # total population size
n_new <- a_new * b  # new sample size

# Calculating new standard error
fpc_new <- sqrt((N - n_new)/N)
se_new <- sqrt(var_cluster_means/a_new) * fpc_new

cat("Expected Standard Error with 40 clusters:", se_new, "\n")
```

Ok so looking at the results I got, I found that if one is to double the number of clusters from 20 to 40 while keeping the cluster size at 10 women each, the expected standard error would decrease to approximately 0.027. This is notably smaller than the original standard error of 0.040, which makes sense because one is sampling more clusters. 

The reduction in standard error suggests that increasing the number of clusters would improve the precision of the estimate by about 32%. This improvement in precision demonstrates how sampling more clusters, even while maintaining the same cluster size, can lead to better estimates of the population mean number of pregnant women per cluster.


## c) Note that the mean y bar is a proportion. Based on the sample of 20 clusters [and ignoring the ratio n / (n â€“ 1)], compute the design effect deff, as well as roh. How would you interpret the design effect for a colleague in plain English?

For cluster sampling, the design effect is:

$$deff = \frac{var_{cluster}(\bar{y})}{var_{srs}(\bar{y})}$$

where:
- $var_{cluster}(\bar{y})$ is the variance under cluster sampling
- $var_{srs}(\bar{y}) = \frac{p(1-p)}{n}$ is the variance under SRS
- $p$ is the proportion (mean)
- $n$ is total sample size

And the intraclass correlation (roh) is:

$$roh = \frac{deff - 1}{b - 1}$$

where:
- $b$ is the cluster size
- $deff$ is the design effect

Now calculating:

Proportion: $$p = \frac{87}{200} = 0.435$$

Cluster sampling variance: $$var_{cluster}(\bar{y}) = \frac{0.0326}{20} = 0.00163$$

SRS variance: $$var_{srs}(\bar{y}) = \frac{0.435(1-0.435)}{200} = 0.00114$$

Design Effect: $$deff = \frac{0.00163}{0.00114} = 1.425$$

Intraclass Correlation: $$roh = \frac{1.425 - 1}{10 - 1} = 0.0472$$

```{r}
# Data
y_alpha <- c(4, 4, 3, 6, 4, 6, 3, 4, 4, 1, 1, 8, 3, 3, 5, 6, 4, 5, 8, 5)
a <- 20      # number of clusters
b <- 10      # cluster size
n <- a * b   # total sample size

# Calculating proportion (p)
p <- sum(y_alpha)/(n)  # divide by ab*b since y_alpha are totals

# Calculating variances
var_cluster <- var(y_alpha/b)/a  # cluster sampling variance
var_srs <- (p*(1-p))/n          # SRS variance

# Calculate deff and roh
deff <- var_cluster/var_srs
roh <- (deff - 1)/(b - 1)

cat("Design Effect (deff):", deff, "\n")
cat("Intraclass Correlation (roh):", roh, "\n")
```

Inferring this in layman terms, the design effect of 1.425 means that because of clustering, the variance is 42.5% higher than if one had sampled individuals completely at random. In other words, the effective sample size is reduced, and observations within the same cluster are somewhat similar; however, the intraclass correlation of about 4.7% indicates that this similarity is relatively low.

## d) Now, using the computed value of roh from part (c), estimate the standard error that you would expect from a sample of a = 40 clusters of b = 5 women each.

For a new design with different cluster size and number, we can use roh to calculate the design effect:

$$deff_{new} = 1 + (b_{new} - 1)roh$$

Then the new standard error is:

$$SE_{new} = \sqrt{\frac{p(1-p)}{n_{new}}} \times \sqrt{deff_{new}} \times \sqrt{\frac{N-n_{new}}{N}}$$

where:
- $roh = 0.0472$ (from part c)
- $b_{new} = 5$ (new cluster size)
- $a_{new} = 40$ (new number of clusters)
- $n_{new} = a_{new} \times b_{new}$ (new total sample size)
- $p = 0.435$ (proportion from original data)
- $N = 2920$ (population size)

so the calculation is :
New design effect: $$deff_{new} = 1 + (5 - 1)(0.0472) = 1.189$$

SRS variance: $$var_{srs} = \frac{0.435(1-0.435)}{200} = 0.00123$$

Finite population correction: $$fpc = \sqrt{\frac{2920-200}{2920}} = 0.917$$

New standard error: $$SE_{new} = \sqrt{0.00123 \times 1.189} \times 0.917 = 0.0352$$

```{r}
# Parameters from previous calculations
roh <- 0.0472      # intraclass correlation from part c
p <- 87/200        # proportion from original data
N <- 2920          # population size

# New design parameters
a_new <- 40        # new number of clusters
b_new <- 5         # new cluster size
n_new <- a_new * b_new

# Calculate new design effect
deff_new <- 1 + (b_new - 1) * roh

# Calculate new standard error
var_srs_new <- (p * (1-p))/n_new
fpc_new <- sqrt((N - n_new)/N)
se_new <- sqrt(var_srs_new * deff_new) * fpc_new

cat("New Design Effect:", deff_new, "\n")
cat("Expected Standard Error:", se_new, "\n")
```

So for the modified cluster design, I found that by changing to 40 clusters of 5 women each (instead of 10 women), the design effect decreased to approximately 1.19. This means the variance penalty for using clusters is now only about 19% higher than simple random sampling, much better than the previous 42.5%. The expected standard error with this new design is about 0.037, which is slightly higher than what I got with 40 clusters of 10 women each (0.027), but still better than the original design of 20 clusters of 10 women (0.040). 

This makes sense because while I increased the number of clusters (which helps precision), we decreased the cluster size, resulting in a smaller total sample size. The trade-off seems reasonable though, as I am getting good precision with fewer total observations.

## 2) The data set, cherry.csv, contains measurements of diameter (inches), height (feet), and timber volume (cubic feet) for a sample of 31 black cherry trees. Diameter and height of trees are easily measured, but volume is more difficult to measure.

## a) Plot volume vs. diameter for the 31 trees.

```{r}
# Reading the data
cherry <- read.csv("cherry.csv")

# Creating scatter plot
plot(volume ~ diameter, data = cherry,
     xlab = "Diameter (inches)",
     ylab = "Volume (cubic feet)",
     main = "Volume vs. Diameter of Black Cherry Trees",
     pch = 19,  # solid circle points
     col = "darkgreen")  # green color for points
     
# Adding grid for better readability
grid()
```
Now the scatter plot visually demonstrates how tree volume increases with diameter. The trend appears non-linear, suggesting that larger diameters correspond to disproportionately higher volumes, which is consistent with biological growth patterns.

## b) Suppose that these trees are an SRS from a forest of N = 2967 trees and that the sum of the diameters for all trees in the forest is 41,835 inches. Use ratio estimation to estimate the total volume for all trees in the forest. Give a 95% CI.

For ratio estimation, I will use:

$$\hat{R} = \frac{\sum_{i=1}^n y_i}{\sum_{i=1}^n x_i}$$

where:
- $y_i$ is the volume of tree i
- $x_i$ is the diameter of tree i

Then the estimated total is:

$$\hat{Y} = \hat{R} \times X$$

The variance of the ratio estimate is:

$$V(\hat{Y}) = N^2(1-\frac{n}{N})\frac{1}{n(n-1)}\sum_{i=1}^n(y_i-\hat{R}x_i)^2$$

The calculation is as follows:

Ratio estimate: 
$$\hat{R} = \frac{\sum y_i}{\sum x_i} = \frac{635.1}{279.3} = 2.275$$

Estimated total volume: 
$$\hat{Y} = 2.275 \times 41,835 = 95,272.16\text{ cubic feet}$$

Standard error: 
$$SE(\hat{Y}) = \sqrt{2967^2(1-\frac{31}{2967})\frac{1}{31(30)}\sum(y_i-2.275x_i)^2} = 5,140.93$$

95% Confidence Interval: 
$$95,272.16 \pm 2.042 \times 5,140.93 = (84,773, 105,771)$$

```{r}
# Setting up the data first
cherry <- read.csv("/Users/namomac/Downloads/cherry.csv")
n <- nrow(cherry)      # sample size
N <- 2967             # population size
X <- 41835           # population total of diameters

# Calculating ratio estimate
R_hat <- sum(cherry$volume)/sum(cherry$diameter)
Y_hat <- R_hat * X

# Calculating standard error
deviations <- cherry$volume - R_hat * cherry$diameter
var_Y <- N^2 * (1-n/N) * sum(deviations^2)/(n*(n-1))
se_Y <- sqrt(var_Y)

# Calculating 95% CI
t_value <- qt(0.975, df=n-1)
ci_lower <- Y_hat - t_value * se_Y
ci_upper <- Y_hat + t_value * se_Y

cat("Ratio estimate of total volume:", Y_hat, "cubic feet\n")
cat("Standard Error:", se_Y, "\n")
cat("95% CI:", c(ci_lower, ci_upper), "\n")
```
Looking at the analysis of the black cherry trees data, I estimated that the total volume of all 2,967 trees in the forest is approximately 95,272 cubic feet. 

To account for sampling uncertainty, I calculated a standard error of about 5,141 cubic feet. This led me to construct a 95% confidence interval ranging from about 84,773 to 105,771 cubic feet. 
This means I can be 95% confident that the true total volume of all trees in the forest falls within this range. The relatively wide confidence interval (spanning about 21,000 cubic feet) reflects the natural variability in tree volumes, even among trees with similar diameters.

## c) Use regression estimation to estimate the total volume for all trees in the forest. Give a 95% CI.

For regression estimation, I will use:

$$\hat{Y}_{reg} = N\bar{y} + b(X - N\bar{x})$$

where:
- $b$ is the slope from regression of $y$ on $x$
- $\bar{y}$ is the sample mean of volume
- $\bar{x}$ is the sample mean of diameter
- $X$ is the population total of diameters
- $N$ is the population size

The variance is:

$$V(\hat{Y}_{reg}) = N^2(1-\frac{n}{N})\frac{s_e^2}{n}$$

where $s_e^2$ is the mean square error from the regression.

So the calculation is:

Sample means: $$\bar{x} = 13.25\text{ inches}, \quad \bar{y} = 20.49\text{ cubic feet}$$

Regression slope: $$b = 4.708$$

Regression estimate: $$\hat{Y}_{reg} = 2967(20.49) + 4.708(41835 - 2967(13.25)) = 102,318.9$$

Standard error: $$SE(\hat{Y}_{reg}) = \sqrt{2967^2(1-\frac{31}{2967})\frac{s_e^2}{31}} = 2,254.0$$

95% Confidence Interval: $$102,318.9 \pm 2.045 \times 2,254.0 = (97,709.0, 106,928.7)$$

```{r}
# Setting up the data
cherry <- read.csv("/Users/namomac/Downloads/cherry.csv")
n <- nrow(cherry)      # sample size
N <- 2967             # population size
X <- 41835           # population total of diameters

# Calculating means
x_bar <- mean(cherry$diameter)
y_bar <- mean(cherry$volume)

# Fitting regression model
reg <- lm(volume ~ diameter, data = cherry)
b <- coef(reg)[2]    # slope

# Calculating regression estimate
Y_reg <- N*y_bar + b*(X - N*x_bar)

# Calculating standard error
MSE <- sum(reg$residuals^2)/(n-2)  # mean square error
var_reg <- N^2 * (1-n/N) * MSE/n
se_reg <- sqrt(var_reg)

# Calculating 95% CI
t_value <- qt(0.975, df=n-2)
ci_lower <- Y_reg - t_value * se_reg
ci_upper <- Y_reg + t_value * se_reg

cat("Regression estimate of total volume:", Y_reg, "cubic feet\n")
cat("Standard Error:", se_reg, "\n")
cat("95% CI:", c(ci_lower, ci_upper), "\n")
```

Now using regression estimation, I found that the total volume of all 2,967 trees in the forest is approximately 102,319 cubic feet. The standard error of this estimate is about 2,254 cubic feet, which is notably smaller than what I got with the ratio estimation (5,141 cubic feet). 

This improved precision is reflected in the narrower 95% confidence interval, ranging from about 97,709 to 106,929 cubic feet. This means I can be 95% confident that the true total volume falls within this roughly 9,220 cubic foot range. 

The smaller standard error and narrower confidence interval suggest that regression estimation is more efficient than ratio estimation for this particular dataset, likely because it better captures the relationship between tree diameter and volume.
